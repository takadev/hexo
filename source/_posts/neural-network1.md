---
title: ニューラルネットワーク入門
date: 2016-12-09 00:00:00
tags:
- "ディープラーニング"
- "python"
category: ディープラーニング
---
## ニューラルネットワークの実装

今回も「ゼロから作るディープラーニング」をもとにニューラルネットワークの実装をPythonでお行いたいと思います。
<!-- More -->

## 入力2出力3のニューラルネットワーク

NumPyの行列計算を使用して、入力が2つ、出力が3つのニューラルネットワークを実装してみます。
ここではバイアスと活性化関数は一旦置いておきます。
Xは入力、Wは重み、Yは出力を示しています。

```
In [1]: import numpy as np
In [2]: X = np.array([1,2])
In [3]: X.shape
Out[3]: (2,)
In [4]: W = np.array([[1,3,5],[2,4,6]])
In [5]: print(W)
[[1 3 5]
 [2 4 6]]
In [6]: W.shape
Out[6]: (2, 3)
In [7]: Y = np.dot(X, W)
In [8]: print(Y)
[ 5 11 17]
```

Xの入力は2つの要素を持つ1次元配列で、重みのWが2x3の配列となり、出力Yが3つの要素を持つ1次元配列になっていることがわかります。
これは入力が2つで出力が3つあるニューラルネットワークになります。

## 3層構造のニューラルネットワーク
次にバイアスと活性化関数を考慮した3層構造のニューラルネットワークを実装してみます。
Bをバイアス、A1は行列の内積を示しています。

```
In [1]: import numpy as np
In [2]: X = np.array([1.0, 0.5])
In [3]: W1 = np.array([[0.1, 0.3, 0.5],[0.2, 0.4, 0.6]])
In [4]: B1 = np.array([0.1, 0.2, 0.3])
In [5]: X.shape
Out[5]: (2,)
In [6]: W1.shape
Out[6]: (2, 3)
In [7]: B1.shape
Out[7]: (3,)
In [8]: A1 = np.dot(X, W1) + B1
```

ここまでで0層目から1層目への入力ができました。次にノード内で行われる活性化関数の実装を行いたいと思います。本に従ってシグモイド関数を使用します。
Z1は1層目の出力を示しています。

```
In [9]: def sigmoid(x):
   ...:     return 1 / (1 + np.exp(-x))
   ...: 
In [10]: Z1 = sigmoid(A1)
In [11]: print(A1)
[ 0.3  0.7  1.1]
In [12]: print(Z1)
[ 0.57444252  0.66818777  0.75026011]
```

次に1層目から2層目への入力を行います。
W2が1層目から2層目への重みで、B2がバイアス、Z1が入力、A2が内積を示しています。

```
In [13]: W2 = np.array([[0.1, 0.4],[0.2, 0.5],[0.3, 0.6]])
In [14]: B2 = np.array([0.1, 0.2])
In [15]: Z1.shape
Out[15]: (3,)
In [16]: W2.shape
Out[16]: (3, 2)
In [17]: B2.shape
Out[17]: (2,)
In [18]: A2 = np.dot(Z1, W2) + B2
In [19]: Z2 = sigmoid(A2)
```

0層目から1層目に行ったこととほぼ同じですね。最後に2層目から3層目の実装です。
予想ができると思いますが、3層目の計算も今までの0層目から1層目、1層目から2層目と同じようなイメージです。

```
In [20]: def identity_function(x):
   ....:     return x
   ....: 
In [21]: W3 = np.array([[0.1, 0.3],[0.2, 0.4]])
In [22]: B3 = np.array([0.1, 0.2])
In [23]: A3 = np.dot(Z2, W3) + B3
In [24]: Y = identity_function(A3)
```

2層目の実装と少し違うところがあると思います。
それは**identity_function()**を定義して活性化関数として最後のノードに使用しているところです。
恒等関数と言って入力をそのまま出力する関数です。最後のノードでなぜ恒等関数を使用するかというと、問題の性質に応じてこの関数を変えて欲しいからです。
今回は適当なデータでサンプルを実装した為、恒等関数を使用しただけのことです。

終わり。
